{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pymorphy3\n",
    "directory = 'YaKlass'\n",
    "os.chdir(directory)\n",
    "morph = pymorphy3.MorphAnalyzer(lang='ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_soup(path):\n",
    "    with open(path, 'r', encoding = 'utf-8') as file:\n",
    "        return BeautifulSoup(file.read(), \"html.parser\")\n",
    "\n",
    "def write(path, text):\n",
    "    with open(path, 'w', encoding = 'utf-8') as file:\n",
    "        file.write(str(text))\n",
    "\n",
    "def term_from_sent(sent, sep = '— это '):\n",
    "    left_side_sent = sent.split(sep)[0]\n",
    "    right_side_sent = sent.split(sep)[1]\n",
    "    term = (left_side_sent, right_side_sent)[len(left_side_sent) > len(right_side_sent)] #выделяет наименьшую по количеству слов часть предложения, которая разбивается '— это'\n",
    "    #if len(left_side_sent) > len(right_side_sent):\n",
    "        #term = right_side_sent\n",
    "    #term = [word.lower() for word in word_tokenize(term) if word not in stops and word != '.']\n",
    "    return term\n",
    "\n",
    "def concat(terms):\n",
    "    terms = [' '.join(term) for term in terms]\n",
    "    for ind, term in enumerate(terms):\n",
    "        if term[1] == ' ':\n",
    "            terms[ind] = term[0] + term[2:]\n",
    "    return terms\n",
    "\n",
    "def text_from_soup(soup):\n",
    "    '''Извлекает весь текст во всех тэгах элемента soup в строку'''\n",
    "    sents_by_strip = \" \".join(item.strip() for item in soup.find_all(text=True)) #находит все тэги с текстом внутри\n",
    "    sents_by_strip = re.sub(\"\\s+\", \" \", sents_by_strip)\n",
    "    return sents_by_strip\n",
    "\n",
    "def score(word):\n",
    "    return morph.parse(word)[0].score\n",
    "\n",
    "def merging_broken_splitted_words(words):\n",
    "    '''объединяет слова, например, \"Т\" и \"реугольник\" объединятся в \"Треугольник\"\n",
    "    работает всегда, кроме случаев, когда идут 2 и более однобуквенных слова'''\n",
    "    #print('words ', words)\n",
    "    if (',' in words) or ('l' in words) or ('m' in words): # сложный случай, будет обработка вручную\n",
    "        return None\n",
    "    \n",
    "    new_words = []\n",
    "    words_was_joined = False\n",
    "    for ind, word in enumerate(words):\n",
    "        if ind == len(words) - 1:\n",
    "            if words_was_joined == False:\n",
    "                new_words.append(word)\n",
    "            break\n",
    "        if len(word) == 1:\n",
    "            new_word = words[ind] + words[ind + 1]\n",
    "            if score(new_word) > score(words[ind + 1]):\n",
    "                new_words.append(new_word)\n",
    "                words_was_joined = True\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        elif words_was_joined == False:\n",
    "            new_words.append(word)\n",
    "        else:\n",
    "            words_was_joined = False\n",
    "    return new_words\n",
    "\n",
    "def term_keyword_count(sent, params = ['—', 'называть', 'называться', 'является', 'являться']):\n",
    "    acceptable_values = ['—', 'называть', 'называться', 'является', 'являться']\n",
    "    for param_ in params:\n",
    "        if param_ not in acceptable_values:\n",
    "            print('Недопустимое keyword для извлечения терминов')\n",
    "            return\n",
    "    count = 0\n",
    "    words = [morph.parse(word)[0].normal_form for word in word_tokenize(sent)]\n",
    "    for param_ in params:\n",
    "        count += words.count(param_)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [path for path in os.listdir() if ('.html' in path) and ('new' not in path)]\n",
    "for path in paths:\n",
    "    with open(path, 'r', encoding = 'utf-8') as file:\n",
    "        lesson = file.read()\n",
    "    \n",
    "    soup = BeautifulSoup(lesson, \"html.parser\")\n",
    "        \n",
    "    meta_tags = soup.find_all('meta') #Удаление всех meta-тэгов\n",
    "    for meta_tag in meta_tags:\n",
    "        meta_tag.decompose()\n",
    "        \n",
    "    reference_tags = soup.find_all(class_ = 'gxst-reference') #удаление нитжнего колонтитула\n",
    "    for reference_tag in reference_tags:\n",
    "        reference_tag.decompose()\n",
    "    \n",
    "    titles = soup.find_all(class_ = 'gxst-title') #Ставим точку в конце каждого div с классом gxst-title\n",
    "    if titles != []:\n",
    "        for title_tag in titles:\n",
    "                \n",
    "            title_text = re.sub(\"\\s+\", \" \", title_tag.text)\n",
    "            title_text = title_text[1:-1]\n",
    "            if (title_text[:-1] != '.') or (title_text[:-1] != '?'):\n",
    "                title_text += '.'\n",
    "            \n",
    "            title_tag.string = title_text\n",
    "                \n",
    "            print(title_tag) #Выводим новые значения div-ов с классом gxst-title\n",
    "        \n",
    "    soup = re.sub(\"(<!--.*?-->)\", \"\", soup.prettify(), flags=re.DOTALL) #Удаляем комментарии\n",
    "    soup = soup.replace('Ваш браузер не поддерживает HTML5 видео', 'Ваш браузер не поддерживает HTML5 видео.') #Ставим точку в конце предложения\n",
    "        \n",
    "    with open(path[:-5] + '_new.txt', 'w', encoding = 'utf-8') as file_2:\n",
    "        file_2.write(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents_by_strip = []\n",
    "new_paths = [path for path in os.listdir() if 'new' in path]\n",
    "for path in new_paths:  \n",
    "    soup = read_soup(path)\n",
    "    \n",
    "    # Сохранение математических тэгов math в текстовый файл\n",
    "    math_tags = soup.find_all('math')\n",
    "    write(path.rsplit('_', maxsplit = 1)[0] + '_math.txt', math_tags)\n",
    "    \n",
    "    # Замена математических тэгов math на символ \"M\"\n",
    "    for tag in soup.find_all('math'):\n",
    "        tag.replace_with('m')\n",
    "\n",
    "    # Сохранение математических формул \\( \\) в текстовый файл\n",
    "    s = str(soup)\n",
    "    matches = [match[0] for match in re.finditer(r'\\\\\\([^\\\\]{0,}\\\\\\)', s)]\n",
    "    write(path.rsplit('_', maxsplit = 1)[0] + '_slash_math.txt', matches)\n",
    "        \n",
    "    # Замена математических формул \\( \\) на символ \"L\"\n",
    "    for match in matches:\n",
    "        s = s.replace(match, 'l')\n",
    "    \n",
    "    # Запись упрощенного результата в _simple.txt файлы\n",
    "    soup = BeautifulSoup(s, \"html.parser\")\n",
    "    write(path.rsplit('_', maxsplit = 1)[0] + '_simple.txt', soup.prettify())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
